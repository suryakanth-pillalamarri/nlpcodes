{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Surya files\\NLP\\nlpcodes\\input.txt\n"
     ]
    }
   ],
   "source": [
    "file_path=r\"D:\\Surya files\\NLP\\nlpcodes\\input.txt\"\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path,'r',encoding='utf-8') as file:\n",
    "    file_contents=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Raw Text Data Example:\n",
      "\n",
      "\"In today's fast-paced world, artificial intelligence (AI) and natural language processing (NLP) play crucial roles. NLP enables machines to understand human language, process text, and communicate effectively. Sentiment analysis algorithms gauge public opinion on social media platforms, helping brands understand customer feedback. Named entity recognition (NER) identifies entities like names (John Doe, Alice), dates (January 1st, 2023), and locations (New York, Paris) in text, crucial for information extraction. Machine translation allows instant communication across languages (e.g., English, Español, 中文), facilitating global interactions. Text summarization condenses lengthy documents into concise summaries, aiding in content digestion. Information retrieval systems index and retrieve relevant documents based on user queries, enhancing search efficiency. Preprocessing involves tokenization, splitting text into meaningful units; stopwords removal; stemming to reduce words to their root form; and vectorization, converting text into numerical representations for machine learning models. Contact us at +1 (123) 456-7890 for more information.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tokenization\n",
    "Tokenization is the process of breaking down text into smaller \n",
    "units called tokens, which can be words, phrases, or symbols.\n",
    "It is a fundamental step in natural language processing (NLP) that \n",
    "prepares text data for further analysis by segmenting it into manageable pieces.\n",
    "\n",
    "Punkt is a pre-trained tokenizer model included in the Natural Language Toolkit (NLTK) library, designed to segment text into sentences and words. It is unsupervised, meaning it can be used to tokenize text in various languages without requiring extensive training data, making it highly adaptable for diverse NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Updated', 'Raw', 'Text', 'Data', 'Example', ':', \"''\", 'In', 'today', \"'s\", 'fast-paced', 'world', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', 'play', 'crucial', 'roles', '.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language', ',', 'process', 'text', ',', 'and', 'communicate', 'effectively', '.', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'on', 'social', 'media', 'platforms', ',', 'helping', 'brands', 'understand', 'customer', 'feedback', '.', 'Named', 'entity', 'recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'names', '(', 'John', 'Doe', ',', 'Alice', ')', ',', 'dates', '(', 'January', '1st', ',', '2023', ')', ',', 'and', 'locations', '(', 'New', 'York', ',', 'Paris', ')', 'in', 'text', ',', 'crucial', 'for', 'information', 'extraction', '.', 'Machine', 'translation', 'allows', 'instant', 'communication', 'across', 'languages', '(', 'e.g.', ',', 'English', ',', 'Español', ',', '中文', ')', ',', 'facilitating', 'global', 'interactions', '.', 'Text', 'summarization', 'condenses', 'lengthy', 'documents', 'into', 'concise', 'summaries', ',', 'aiding', 'in', 'content', 'digestion', '.', 'Information', 'retrieval', 'systems', 'index', 'and', 'retrieve', 'relevant', 'documents', 'based', 'on', 'user', 'queries', ',', 'enhancing', 'search', 'efficiency', '.', 'Preprocessing', 'involves', 'tokenization', ',', 'splitting', 'text', 'into', 'meaningful', 'units', ';', 'stopwords', 'removal', ';', 'stemming', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', ';', 'and', 'vectorization', ',', 'converting', 'text', 'into', 'numerical', 'representations', 'for', 'machine', 'learning', 'models', '.', 'Contact', 'us', 'at', '+1', '(', '123', ')', '456-7890', 'for', 'more', 'information', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(file_contents)\n",
    "print('Tokens in the given input')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stop word removal\n",
    "\n",
    "\n",
    "Stop words are common words in a language, such as \"and,\" \"the,\" \"is,\" and \"in,\" that are often filtered out during text processing. They are typically removed from text data to focus on the more meaningful words that contribute to the main content and context, as stop words generally do not carry significant semantic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Updated', 'Raw', 'Text', 'Data', 'Example', ':', \"''\", 'today', \"'s\", 'fast-paced', 'world', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'natural', 'language', 'processing', '(', 'NLP', ')', 'play', 'crucial', 'roles', '.', 'NLP', 'enables', 'machines', 'understand', 'human', 'language', ',', 'process', 'text', ',', 'communicate', 'effectively', '.', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'social', 'media', 'platforms', ',', 'helping', 'brands', 'understand', 'customer', 'feedback', '.', 'Named', 'entity', 'recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'names', '(', 'John', 'Doe', ',', 'Alice', ')', ',', 'dates', '(', 'January', '1st', ',', '2023', ')', ',', 'locations', '(', 'New', 'York', ',', 'Paris', ')', 'text', ',', 'crucial', 'information', 'extraction', '.', 'Machine', 'translation', 'allows', 'instant', 'communication', 'across', 'languages', '(', 'e.g.', ',', 'English', ',', 'Español', ',', '中文', ')', ',', 'facilitating', 'global', 'interactions', '.', 'Text', 'summarization', 'condenses', 'lengthy', 'documents', 'concise', 'summaries', ',', 'aiding', 'content', 'digestion', '.', 'Information', 'retrieval', 'systems', 'index', 'retrieve', 'relevant', 'documents', 'based', 'user', 'queries', ',', 'enhancing', 'search', 'efficiency', '.', 'Preprocessing', 'involves', 'tokenization', ',', 'splitting', 'text', 'meaningful', 'units', ';', 'stopwords', 'removal', ';', 'stemming', 'reduce', 'words', 'root', 'form', ';', 'vectorization', ',', 'converting', 'text', 'numerical', 'representations', 'machine', 'learning', 'models', '.', 'Contact', 'us', '+1', '(', '123', ')', '456-7890', 'information', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rmoving special characters and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Updated', 'Raw', 'Text', 'Data', 'Example', 'today', 's', 'fastpaced', 'world', 'artificial', 'intelligence', 'AI', 'natural', 'language', 'processing', 'NLP', 'play', 'crucial', 'roles', 'NLP', 'enables', 'machines', 'understand', 'human', 'language', 'process', 'text', 'communicate', 'effectively', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'social', 'media', 'platforms', 'helping', 'brands', 'understand', 'customer', 'feedback', 'Named', 'entity', 'recognition', 'NER', 'identifies', 'entities', 'like', 'names', 'John', 'Doe', 'Alice', 'dates', 'January', 'st', 'locations', 'New', 'York', 'Paris', 'text', 'crucial', 'information', 'extraction', 'Machine', 'translation', 'allows', 'instant', 'communication', 'across', 'languages', 'eg', 'English', 'Espaol', 'facilitating', 'global', 'interactions', 'Text', 'summarization', 'condenses', 'lengthy', 'documents', 'concise', 'summaries', 'aiding', 'content', 'digestion', 'Information', 'retrieval', 'systems', 'index', 'retrieve', 'relevant', 'documents', 'based', 'user', 'queries', 'enhancing', 'search', 'efficiency', 'Preprocessing', 'involves', 'tokenization', 'splitting', 'text', 'meaningful', 'units', 'stopwords', 'removal', 'stemming', 'reduce', 'words', 'root', 'form', 'vectorization', 'converting', 'text', 'numerical', 'representations', 'machine', 'learning', 'models', 'Contact', 'us', 'information']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokens = [re.sub(r'[^A-Za-z]+', '', token) for token in filtered_tokens if re.sub(r'[^A-Za-z]+', '', token)]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
