{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Surya files\\NLP\\nlpcodes\\input.txt\n"
     ]
    }
   ],
   "source": [
    "file_path=r\"D:\\Surya files\\NLP\\nlpcodes\\input.txt\"\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path,'r',encoding='utf-8') as file:\n",
    "    file_contents=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Raw Text Data Example:\n",
      "\n",
      "\"In today's fast-paced world, artificial intelligence (AI) and natural language processing (NLP) play crucial roles. NLP enables machines to understand human language, process text, and communicate effectively. Sentiment analysis algorithms gauge public opinion on social media platforms, helping brands understand customer feedback. Named entity recognition (NER) identifies entities like names (John Doe, Alice), dates (January 1st, 2023), and locations (New York, Paris) in text, crucial for information extraction. Machine translation allows instant communication across languages (e.g., English, Español, 中文), facilitating global interactions. Text summarization condenses lengthy documents into concise summaries, aiding in content digestion. Information retrieval systems index and retrieve relevant documents based on user queries, enhancing search efficiency. Preprocessing involves tokenization, splitting text into meaningful units; stopwords removal; stemming to reduce words to their root form; and vectorization, converting text into numerical representations for machine learning models. Contact us at +1 (123) 456-7890 for more information.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tokenization\n",
    "Tokenization is the process of breaking down text into smaller \n",
    "units called tokens, which can be words, phrases, or symbols.\n",
    "It is a fundamental step in natural language processing (NLP) that \n",
    "prepares text data for further analysis by segmenting it into manageable pieces.\n",
    "\n",
    "Punkt is a pre-trained tokenizer model included in the Natural Language Toolkit (NLTK) library, designed to segment text into sentences and words. It is unsupervised, meaning it can be used to tokenize text in various languages without requiring extensive training data, making it highly adaptable for diverse NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in the given input\n",
      "['Updated', 'Raw', 'Text', 'Data', 'Example', ':', \"''\", 'In', 'today', \"'s\", 'fast-paced', 'world', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', 'play', 'crucial', 'roles', '.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language', ',', 'process', 'text', ',', 'and', 'communicate', 'effectively', '.', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'on', 'social', 'media', 'platforms', ',', 'helping', 'brands', 'understand', 'customer', 'feedback', '.', 'Named', 'entity', 'recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'names', '(', 'John', 'Doe', ',', 'Alice', ')', ',', 'dates', '(', 'January', '1st', ',', '2023', ')', ',', 'and', 'locations', '(', 'New', 'York', ',', 'Paris', ')', 'in', 'text', ',', 'crucial', 'for', 'information', 'extraction', '.', 'Machine', 'translation', 'allows', 'instant', 'communication', 'across', 'languages', '(', 'e.g.', ',', 'English', ',', 'Español', ',', '中文', ')', ',', 'facilitating', 'global', 'interactions', '.', 'Text', 'summarization', 'condenses', 'lengthy', 'documents', 'into', 'concise', 'summaries', ',', 'aiding', 'in', 'content', 'digestion', '.', 'Information', 'retrieval', 'systems', 'index', 'and', 'retrieve', 'relevant', 'documents', 'based', 'on', 'user', 'queries', ',', 'enhancing', 'search', 'efficiency', '.', 'Preprocessing', 'involves', 'tokenization', ',', 'splitting', 'text', 'into', 'meaningful', 'units', ';', 'stopwords', 'removal', ';', 'stemming', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', ';', 'and', 'vectorization', ',', 'converting', 'text', 'into', 'numerical', 'representations', 'for', 'machine', 'learning', 'models', '.', 'Contact', 'us', 'at', '+1', '(', '123', ')', '456-7890', 'for', 'more', 'information', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(file_contents)\n",
    "print('Tokens in the given input')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stop word removal\n",
    "\n",
    "\n",
    "Stop words are common words in a language, such as \"and,\" \"the,\" \"is,\" and \"in,\" that are often filtered out during text processing. They are typically removed from text data to focus on the more meaningful words that contribute to the main content and context, as stop words generally do not carry significant semantic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Updated', 'Raw', 'Text', 'Data', 'Example', ':', \"''\", 'today', \"'s\", 'fast-paced', 'world', ',', 'artificial', 'intelligence', '(', 'AI', ')', 'natural', 'language', 'processing', '(', 'NLP', ')', 'play', 'crucial', 'roles', '.', 'NLP', 'enables', 'machines', 'understand', 'human', 'language', ',', 'process', 'text', ',', 'communicate', 'effectively', '.', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'social', 'media', 'platforms', ',', 'helping', 'brands', 'understand', 'customer', 'feedback', '.', 'Named', 'entity', 'recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'names', '(', 'John', 'Doe', ',', 'Alice', ')', ',', 'dates', '(', 'January', '1st', ',', '2023', ')', ',', 'locations', '(', 'New', 'York', ',', 'Paris', ')', 'text', ',', 'crucial', 'information', 'extraction', '.', 'Machine', 'translation', 'allows', 'instant', 'communication', 'across', 'languages', '(', 'e.g.', ',', 'English', ',', 'Español', ',', '中文', ')', ',', 'facilitating', 'global', 'interactions', '.', 'Text', 'summarization', 'condenses', 'lengthy', 'documents', 'concise', 'summaries', ',', 'aiding', 'content', 'digestion', '.', 'Information', 'retrieval', 'systems', 'index', 'retrieve', 'relevant', 'documents', 'based', 'user', 'queries', ',', 'enhancing', 'search', 'efficiency', '.', 'Preprocessing', 'involves', 'tokenization', ',', 'splitting', 'text', 'meaningful', 'units', ';', 'stopwords', 'removal', ';', 'stemming', 'reduce', 'words', 'root', 'form', ';', 'vectorization', ',', 'converting', 'text', 'numerical', 'representations', 'machine', 'learning', 'models', '.', 'Contact', 'us', '+1', '(', '123', ')', '456-7890', 'information', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rmoving special characters and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Updated', 'Raw', 'Text', 'Data', 'Example', 'today', 's', 'fastpaced', 'world', 'artificial', 'intelligence', 'AI', 'natural', 'language', 'processing', 'NLP', 'play', 'crucial', 'roles', 'NLP', 'enables', 'machines', 'understand', 'human', 'language', 'process', 'text', 'communicate', 'effectively', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'social', 'media', 'platforms', 'helping', 'brands', 'understand', 'customer', 'feedback', 'Named', 'entity', 'recognition', 'NER', 'identifies', 'entities', 'like', 'names', 'John', 'Doe', 'Alice', 'dates', 'January', 'st', 'locations', 'New', 'York', 'Paris', 'text', 'crucial', 'information', 'extraction', 'Machine', 'translation', 'allows', 'instant', 'communication', 'across', 'languages', 'eg', 'English', 'Espaol', 'facilitating', 'global', 'interactions', 'Text', 'summarization', 'condenses', 'lengthy', 'documents', 'concise', 'summaries', 'aiding', 'content', 'digestion', 'Information', 'retrieval', 'systems', 'index', 'retrieve', 'relevant', 'documents', 'based', 'user', 'queries', 'enhancing', 'search', 'efficiency', 'Preprocessing', 'involves', 'tokenization', 'splitting', 'text', 'meaningful', 'units', 'stopwords', 'removal', 'stemming', 'reduce', 'words', 'root', 'form', 'vectorization', 'converting', 'text', 'numerical', 'representations', 'machine', 'learning', 'models', 'Contact', 'us', 'information']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokens = [re.sub(r'[^A-Za-z]+', '', token) for token in filtered_tokens if re.sub(r'[^A-Za-z]+', '', token)]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "Reducing the word to their root or base form\n",
    "example running,runner,ran to the base  form run\n",
    "or choping of the prefixes and suffexies.\n",
    "it is less accurate but it is faste.\n",
    "\n",
    "used when i need rough reduction of words to their base for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemed tokens:\n",
      "['updat', 'raw', 'text', 'data', 'exampl', 'today', 's', 'fastpac', 'world', 'artifici', 'intellig', 'ai', 'natur', 'languag', 'process', 'nlp', 'play', 'crucial', 'role', 'nlp', 'enabl', 'machin', 'understand', 'human', 'languag', 'process', 'text', 'commun', 'effect', 'sentiment', 'analysi', 'algorithm', 'gaug', 'public', 'opinion', 'social', 'media', 'platform', 'help', 'brand', 'understand', 'custom', 'feedback', 'name', 'entiti', 'recognit', 'ner', 'identifi', 'entiti', 'like', 'name', 'john', 'doe', 'alic', 'date', 'januari', 'st', 'locat', 'new', 'york', 'pari', 'text', 'crucial', 'inform', 'extract', 'machin', 'translat', 'allow', 'instant', 'commun', 'across', 'languag', 'eg', 'english', 'espaol', 'facilit', 'global', 'interact', 'text', 'summar', 'condens', 'lengthi', 'document', 'concis', 'summari', 'aid', 'content', 'digest', 'inform', 'retriev', 'system', 'index', 'retriev', 'relev', 'document', 'base', 'user', 'queri', 'enhanc', 'search', 'effici', 'preprocess', 'involv', 'token', 'split', 'text', 'meaning', 'unit', 'stopword', 'remov', 'stem', 'reduc', 'word', 'root', 'form', 'vector', 'convert', 'text', 'numer', 'represent', 'machin', 'learn', 'model', 'contact', 'us', 'inform']\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "\n",
    "stemmed_tokens=[stemmer.stem(token) for token in cleaned_tokens]\n",
    "\n",
    "print('Stemed tokens:')\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lematization\n",
    "\n",
    "Text preprocessing technique used in natural preprocesssing to reduce the word to their base form known as lemma.\n",
    "\n",
    "it is more accurate then stemming\n",
    "\n",
    "running to run\n",
    "geese to goose\n",
    "it is useful in tasks such as mining information retrial and natural language under standingto ensure better under standing of the text.\n",
    "used when context and meenin is importent in the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Raw Text Data Example today s fastpaced world artificial intelligence AI natural language processing NLP play crucial roles NLP enables machines understand human language process text communicate effectively Sentiment analysis algorithms gauge public opinion social media platforms helping brands understand customer feedback Named entity recognition NER identifies entities like names John Doe Alice dates January st locations New York Paris text crucial information extraction Machine translation allows instant communication across languages eg English Espaol facilitating global interactions Text summarization condenses lengthy documents concise summaries aiding content digestion Information retrieval systems index retrieve relevant documents based user queries enhancing search efficiency Preprocessing involves tokenization splitting text meaningful units stopwords removal stemming reduce words root form vectorization converting text numerical representations machine learning models Contact us information\n",
      "lematised tokens\n",
      "['update', 'Raw', 'Text', 'Data', 'Example', 'today', 's', 'fastpaced', 'world', 'artificial', 'intelligence', 'AI', 'natural', 'language', 'processing', 'NLP', 'play', 'crucial', 'role', 'NLP', 'enable', 'machine', 'understand', 'human', 'language', 'process', 'text', 'communicate', 'effectively', 'Sentiment', 'analysis', 'algorithms', 'gauge', 'public', 'opinion', 'social', 'medium', 'platform', 'help', 'brand', 'understand', 'customer', 'feedback', 'name', 'entity', 'recognition', 'NER', 'identify', 'entity', 'like', 'name', 'John', 'Doe', 'Alice', 'date', 'January', 'st', 'location', 'New', 'York', 'Paris', 'text', 'crucial', 'information', 'extraction', 'Machine', 'translation', 'allow', 'instant', 'communication', 'across', 'language', 'eg', 'English', 'Espaol', 'facilitate', 'global', 'interaction', 'Text', 'summarization', 'condense', 'lengthy', 'document', 'concise', 'summary', 'aid', 'content', 'digestion', 'information', 'retrieval', 'system', 'index', 'retrieve', 'relevant', 'document', 'base', 'user', 'query', 'enhance', 'search', 'efficiency', 'preprocesse', 'involve', 'tokenization', 'splitting', 'text', 'meaningful', 'unit', 'stopword', 'removal', 'stem', 'reduce', 'word', 'root', 'form', 'vectorization', 'convert', 'text', 'numerical', 'representation', 'machine', 'learning', 'model', 'contact', 'we', 'information']\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(' '.join(cleaned_tokens))\n",
    "\n",
    "print(doc)\n",
    "\n",
    "lematied_tokens=[token.lemma_ for token in doc]\n",
    "\n",
    "print('lematised tokens')\n",
    "print(lematied_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER called as named entity recorginisation is nlp technique to identify and classify the named entities into predefinedcatagories such as names of persons oraganizations\n",
    "locations and dates etc\n",
    "\n",
    "example input:\n",
    "vasaudeva is the supreme lord liing in Varanasi.\n",
    "\n",
    "vasudeva- person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given text\n",
      "Updated Raw Text Data Example today s fastpaced world artificial intelligence AI natural language processing NLP play crucial roles NLP enables machines understand human language process text communicate effectively Sentiment analysis algorithms gauge public opinion social media platforms helping brands understand customer feedback Named entity recognition NER identifies entities like names John Doe Alice dates January st locations New York Paris text crucial information extraction Machine translation allows instant communication across languages eg English Espaol facilitating global interactions Text summarization condenses lengthy documents concise summaries aiding content digestion Information retrieval systems index retrieve relevant documents based user queries enhancing search efficiency Preprocessing involves tokenization splitting text meaningful units stopwords removal stemming reduce words root form vectorization converting text numerical representations machine learning models Contact us information\n",
      "Named entitis\n",
      "today -> DATE\n",
      "NLP -> ORG\n",
      "NLP -> ORG\n",
      "NER -> ORG\n",
      "John Doe Alice -> PERSON\n",
      "January -> DATE\n",
      "New York -> GPE\n",
      "Paris -> GPE\n",
      "Machine -> ORG\n",
      "Espaol -> ORG\n",
      "Preprocessing -> PERSON\n"
     ]
    }
   ],
   "source": [
    "document1=nlp(doc)\n",
    "print('Given text')\n",
    "\n",
    "print(document1)\n",
    "\n",
    "print(\"Named entitis\")\n",
    "\n",
    "for ent in document1.ents:\n",
    "    print(ent.text,'->',ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS taggign is the process of marking up the word in  text as partikclar part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts f speech tagging for the input text :)\n",
      "Updated -> VERB\n",
      "Raw -> PROPN\n",
      "Text -> PROPN\n",
      "Data -> PROPN\n",
      "Example -> PROPN\n",
      "today -> NOUN\n",
      "s -> PART\n",
      "fastpaced -> ADJ\n",
      "world -> NOUN\n",
      "artificial -> ADJ\n",
      "intelligence -> NOUN\n",
      "AI -> PROPN\n",
      "natural -> ADJ\n",
      "language -> NOUN\n",
      "processing -> NOUN\n",
      "NLP -> PROPN\n",
      "play -> VERB\n",
      "crucial -> ADJ\n",
      "roles -> NOUN\n",
      "NLP -> PROPN\n",
      "enables -> VERB\n",
      "machines -> NOUN\n",
      "understand -> VERB\n",
      "human -> ADJ\n",
      "language -> NOUN\n",
      "process -> NOUN\n",
      "text -> NOUN\n",
      "communicate -> VERB\n",
      "effectively -> ADV\n",
      "Sentiment -> PROPN\n",
      "analysis -> NOUN\n",
      "algorithms -> NOUN\n",
      "gauge -> VERB\n",
      "public -> ADJ\n",
      "opinion -> NOUN\n",
      "social -> ADJ\n",
      "media -> NOUN\n",
      "platforms -> NOUN\n",
      "helping -> VERB\n",
      "brands -> NOUN\n",
      "understand -> VERB\n",
      "customer -> NOUN\n",
      "feedback -> NOUN\n",
      "Named -> VERB\n",
      "entity -> NOUN\n",
      "recognition -> NOUN\n",
      "NER -> PROPN\n",
      "identifies -> VERB\n",
      "entities -> NOUN\n",
      "like -> ADP\n",
      "names -> NOUN\n",
      "John -> PROPN\n",
      "Doe -> PROPN\n",
      "Alice -> PROPN\n",
      "dates -> VERB\n",
      "January -> PROPN\n",
      "st -> PROPN\n",
      "locations -> NOUN\n",
      "New -> PROPN\n",
      "York -> PROPN\n",
      "Paris -> PROPN\n",
      "text -> NOUN\n",
      "crucial -> ADJ\n",
      "information -> NOUN\n",
      "extraction -> NOUN\n",
      "Machine -> PROPN\n",
      "translation -> NOUN\n",
      "allows -> VERB\n",
      "instant -> ADJ\n",
      "communication -> NOUN\n",
      "across -> ADP\n",
      "languages -> NOUN\n",
      "eg -> VERB\n",
      "English -> PROPN\n",
      "Espaol -> PROPN\n",
      "facilitating -> VERB\n",
      "global -> ADJ\n",
      "interactions -> NOUN\n",
      "Text -> PROPN\n",
      "summarization -> NOUN\n",
      "condenses -> VERB\n",
      "lengthy -> ADJ\n",
      "documents -> NOUN\n",
      "concise -> NOUN\n",
      "summaries -> NOUN\n",
      "aiding -> VERB\n",
      "content -> NOUN\n",
      "digestion -> NOUN\n",
      "Information -> NOUN\n",
      "retrieval -> NOUN\n",
      "systems -> NOUN\n",
      "index -> NOUN\n",
      "retrieve -> VERB\n",
      "relevant -> ADJ\n",
      "documents -> NOUN\n",
      "based -> VERB\n",
      "user -> NOUN\n",
      "queries -> VERB\n",
      "enhancing -> VERB\n",
      "search -> NOUN\n",
      "efficiency -> NOUN\n",
      "Preprocessing -> VERB\n",
      "involves -> VERB\n",
      "tokenization -> NOUN\n",
      "splitting -> NOUN\n",
      "text -> NOUN\n",
      "meaningful -> ADJ\n",
      "units -> NOUN\n",
      "stopwords -> VERB\n",
      "removal -> NOUN\n",
      "stemming -> VERB\n",
      "reduce -> VERB\n",
      "words -> NOUN\n",
      "root -> NOUN\n",
      "form -> NOUN\n",
      "vectorization -> NOUN\n",
      "converting -> VERB\n",
      "text -> NOUN\n",
      "numerical -> ADJ\n",
      "representations -> NOUN\n",
      "machine -> NOUN\n",
      "learning -> NOUN\n",
      "models -> NOUN\n",
      "Contact -> VERB\n",
      "us -> PRON\n",
      "information -> NOUN\n"
     ]
    }
   ],
   "source": [
    "print(\"Parts of speech tagging for the input text :)\")\n",
    "for token in doc:\n",
    "    print(token.text,'->',token.pos_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
